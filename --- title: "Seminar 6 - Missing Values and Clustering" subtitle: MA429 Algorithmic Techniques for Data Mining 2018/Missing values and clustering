---
title: "Seminar 6 - Missing Values and Clustering"
subtitle: MA429 Algorithmic Techniques for Data Mining 2018/19
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

```{r,include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

# Synthetic Dataset

```{r}
set.seed(47)
x=matrix(rnorm(50*2), ncol=2)
x[1:25,1]=x[1:25,1]+2
x[1:25,2]=x[1:25,2]-2
plot(x)
```

# K-Means Clustering

```{r}
set.seed(1)
km.out=kmeans(x,2)
plot(x, col=(km.out$cluster), main="K-Means Clustering Results with K=2")
```

Note that k-means is sensitive to the initial choice. As a demonstration, try this:

```{r}
set.seed(1)
kmSeed1 = kmeans(x,2,nstart=1)
plot(x, col=(kmSeed1$cluster), main="K-Means Clustering Results with K=2 (seed 1)")
set.seed(6)
kmSeed6 = kmeans(x,2,nstart=1)
plot(x, col=(kmSeed6$cluster), main="K-Means Clustering Results with K=2 (seed 6)")
```

You can get a more consistent result by repeating with several different initial center choices. This can be donoe using the nstart argument to the kmeans functions:

```{r}
set.seed(1)
kmSeed1 = kmeans(x,2,nstart=10)
plot(x, col=(kmSeed1$cluster), main="K-Means Clustering Results with K=2 nstart=10 (seed 1)")
set.seed(6)
kmSeed6 = kmeans(x,2,nstart=10)
plot(x, col=(kmSeed6$cluster), main="K-Means Clustering Results with K=2 nstart=10 (seed 6)")
```

The 'centers' argument can be used to tell the kmeans function how many clusters to find:

```{r}
set.seed(1)
km3 = kmeans(x,3,nstart=10)
plot(x, col=(km3$cluster), main="K-Means Clustering Results with K=3")
```

The within-cluster sum of squares are stored in the list returned by the kmeans function:

```{r}
set.seed(1)
kmSeed1 = kmeans(x,2,nstart=10)
plot(x, col=(kmSeed1$cluster), main="K-Means Clustering Results with K=2 nstart=10 (seed 1)")

kmSeed1$withinss
kmSeed1$totss
kmSeed1$tot.withinss
```

# Hierarchical Clustering

The hclust() function can be used to perform hierarchical clustering in R. In hierarchical clustering all elements beging in their own size-1 clusters. So if you have 50 observations, we begin with 50 clusters. Then a linkage method is used to decide how to iteratively combine clusters. So in the second iteration you have 49 clusters, in the third iteration 48 and so forth, until only a single cluster remains. The linkage method determines how to compute distances between clusters. Here are three such examples:

```{r}
hc.complete = hclust(dist(x), method="complete")
hc.average = hclust(dist(x), method="average")
hc.single = hclust(dist(x), method="single")
```

(Note that the dist(x) function creates a distance matrix between the 50 observations). Because hierarchical creates a division of $n$ elements into clusters for every number of clusters between 1 and $n$, we can visualise this using a dendrogram. Here they are for the three linkage methods above:

```{r}
par(mfrow=c(1,3))
plot(hc.complete,main="Complete Linkage", xlab="", sub="", cex=.9)
plot(hc.average, main="Average Linkage", xlab="", sub="", cex=.9)
plot(hc.single, main="Single Linkage", xlab="", sub="", cex=.9)
```

So in and of itself, hierarchical clustering doesn't yet tell you the cluster assignments of your dataset. You need to decide how many clusters you want, and then cut the tree at that number of clusters. For example:

```{r}
ctComplete = cutree(hc.complete, 2)
ctAverage = cutree(hc.average, 2)
ctSingle = cutree(hc.single, 2)
ctComplete
ctAverage
ctSingle
```

And finally, because a distance matrix is used for computing distances between clusters, keep in mind that scaling is important!

```{r}
xscaled = scale(x)
par(mfrow=c(1,2))
plot(hclust(dist(x), method="average"), main="Hierarchical Clustering")
plot(hclust(dist(xscaled), method="average"), main="Hierarchical Clustering with Scaled Features")
```
